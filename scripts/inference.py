import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import sys
import os

# 1. ëª¨ë¸ ê²½ë¡œ ë° ë¼ë²¨ ì •ì˜
MODEL_DIR = "models/classifier"
labels = ["ê¸°ê°", "ê°í•˜", "ì¸ìš©", "ì¼ë¶€ ì¸ìš©", "ì¡°ì •"]  # í•™ìŠµ ì‹œ ì •ë ¬ëœ ìˆœì„œì— ë§ì¶°ì•¼ í•¨
id2label = {i: label for i, label in enumerate(labels)}

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
model.eval()

# 3. íŒë¡€ ë³¸ë¬¸ ì…ë ¥ ë°›ê¸° (CLI ë˜ëŠ” í…ŒìŠ¤íŠ¸ìš©)
if len(sys.argv) > 1:
    input_text = sys.argv[1]
else:
    input_text = input("íŒë¡€ ë³¸ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:\n")

# 4. í† í¬ë‚˜ì´ì¦ˆ ë° í…ì„œ ë³€í™˜
inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="max_length", max_length=512)

# 5. ëª¨ë¸ ì¶”ë¡ 
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    pred_id = torch.argmax(logits, dim=-1).item()
    pred_label = id2label[pred_id]

# 6. ê²°ê³¼ ì¶œë ¥
print(f"\nğŸ§¾ ì˜ˆì¸¡ëœ íŒê²° ê²°ê³¼: {pred_label}")
