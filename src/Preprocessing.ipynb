{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ notebook ìœ„ì¹˜: scr/\n",
    "BASE_DIR = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "LABEL_CSV_PATH = os.path.join(BASE_DIR, \"data\", \"gpt4.1_label_1000d.csv\")\n",
    "LABEL_MAP_PATH = os.path.join(BASE_DIR, \"data\", \"label_map.json\")\n",
    "RAW_JSON_DIR = os.path.join(BASE_DIR, \"data\", \"raw\", \"ì›ë³¸\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] ë¼ë²¨ íŒŒì¼ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ì‚¬ê±´ë²ˆí˜¸', 'íŒŒì¼ëª…', 'ë¼ë²¨ë§ê²°ê³¼'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# label_map ë¡œë“œ\n",
    "with open(LABEL_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "# ë¼ë²¨ CSV ë¡œë”©\n",
    "df_label = pd.read_csv(LABEL_CSV_PATH)\n",
    "print(df_label.columns)  # \"ì‚¬ê±´ë²ˆí˜¸\", \"ë¼ë²¨ë§ê²°ê³¼\" ê°™ì€ì§€ í™•ì¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target_text(full_text: str) -> str:\n",
    "    party_info = re.search(r\"(ã€ì›ê³ , í”¼ìƒê³ ì¸ã€‘.*?ã€í”¼ê³ , ìƒê³ ì¸ã€‘)\", full_text, re.DOTALL)\n",
    "    judgement_info = re.search(r\"(ã€ì£¼[\\s]*ë¬¸ã€‘.*?)(ã€ì´[\\s]*ìœ ã€‘|ã€ì´ìœ ã€‘)\", full_text, re.DOTALL)\n",
    "\n",
    "    combined = \"\"\n",
    "    if party_info:\n",
    "        combined += party_info.group(1).strip() + \"\\n\"\n",
    "    if judgement_info:\n",
    "        combined += judgement_info.group(1).strip()\n",
    "    return combined.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] ì›ë³¸ JSON ìˆœíšŒ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101d894023b14b3bb422aa51f2117de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì´ usable ìƒ˜í”Œ ìˆ˜: 843\n",
      "\n",
      "âŒ regex_fail_fallback: 3ê±´\n",
      "\n",
      "â— fallback ì²˜ë¦¬ëœ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (ì •ê·œì‹ ì‹¤íŒ¨)\n",
      "- 213723.json\n",
      "- 216653.json\n",
      "- 223575.json\n",
      "\n",
      "ğŸ“ fallback ì²˜ë¦¬ ë¦¬ìŠ¤íŠ¸ ì €ì¥ë¨: c:\\dev\\github\\SKN10-FINAL-3Team\\data\\processed\\regex_fallback_list.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_list = []\n",
    "missing_stats = defaultdict(list)\n",
    "\n",
    "def extract_target_text_with_fallback(full_text: str, fallback_len: int = 1000) -> str:\n",
    "    import re\n",
    "    party_info = re.search(r\"(ã€ì›ê³ , í”¼ìƒê³ ì¸ã€‘.*?ã€í”¼ê³ , ìƒê³ ì¸ã€‘)\", full_text, re.DOTALL)\n",
    "    judgement_info = re.search(r\"(ã€ì£¼[\\s]*ë¬¸ã€‘.*?)(ã€ì´[\\s]*ìœ ã€‘|ã€ì´ìœ ã€‘)\", full_text, re.DOTALL)\n",
    "\n",
    "    combined = \"\"\n",
    "    if party_info:\n",
    "        combined += party_info.group(1).strip() + \"\\n\"\n",
    "    if judgement_info:\n",
    "        combined += judgement_info.group(1).strip()\n",
    "\n",
    "    if combined.strip():\n",
    "        return combined.strip()\n",
    "    else:\n",
    "        return full_text[:fallback_len].strip()  # fallback ì‚¬ìš©\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì‹œì‘\n",
    "for idx, row in tqdm(df_label.iterrows(), total=len(df_label)):\n",
    "    filename = str(row[\"íŒŒì¼ëª…\"]).strip()\n",
    "    label_str = row[\"ë¼ë²¨ë§ê²°ê³¼\"].strip()\n",
    "    label = label_map.get(label_str, None)\n",
    "\n",
    "    if label is None:\n",
    "        missing_stats[\"label_map_error\"].append(filename)\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(RAW_JSON_DIR, filename)\n",
    "    if not os.path.exists(json_path):\n",
    "        missing_stats[\"file_missing\"].append(filename)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            case_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        missing_stats[\"json_read_error\"].append(filename)\n",
    "        continue\n",
    "\n",
    "    full_text = case_data.get(\"íŒë¡€ë‚´ìš©\", \"\")\n",
    "    if not full_text:\n",
    "        missing_stats[\"no íŒë¡€ë‚´ìš©\"].append(filename)\n",
    "        continue\n",
    "\n",
    "    trimmed_text = extract_target_text_with_fallback(full_text)\n",
    "    if trimmed_text.strip() == full_text[:1000].strip():\n",
    "        missing_stats[\"regex_fail_fallback\"].append(filename)\n",
    "\n",
    "    data_list.append({\n",
    "        \"text\": trimmed_text,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… ì´ usable ìƒ˜í”Œ ìˆ˜: {len(data_list)}\\n\")\n",
    "\n",
    "for reason, files in missing_stats.items():\n",
    "    print(f\"âŒ {reason}: {len(files)}ê±´\")\n",
    "\n",
    "# regex_fail_fallback íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ (ìµœëŒ€ 10ê°œë§Œ ë³´ê¸°)\n",
    "print(\"\\nâ— fallback ì²˜ë¦¬ëœ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (ì •ê·œì‹ ì‹¤íŒ¨)\")\n",
    "for f in missing_stats[\"regex_fail_fallback\"][:10]:\n",
    "    print(\"-\", f)\n",
    "\n",
    "# í•„ìš”í•˜ë©´ ì „ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
    "fallback_list_path = os.path.join(OUTPUT_DIR, \"regex_fallback_list.txt\")\n",
    "with open(fallback_list_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "    for f in missing_stats[\"regex_fail_fallback\"]:\n",
    "        fw.write(f + \"\\n\")\n",
    "\n",
    "print(f\"\\nğŸ“ fallback ì²˜ë¦¬ ë¦¬ìŠ¤íŠ¸ ì €ì¥ë¨: {fallback_list_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] 7:2:1 ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ë²¨ 7: 117ê°œ\n",
      "ë¼ë²¨ 4: 209ê°œ\n",
      "ë¼ë²¨ 0: 133ê°œ\n",
      "ë¼ë²¨ 5: 62ê°œ\n",
      "ë¼ë²¨ 2: 151ê°œ\n",
      "ë¼ë²¨ 1: 166ê°œ\n",
      "ë¼ë²¨ 3: 4ê°œ\n",
      "ë¼ë²¨ 6: 1ê°œ\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter([x[\"label\"] for x in data_list])\n",
    "for label_id, count in label_counts.items():\n",
    "    print(f\"ë¼ë²¨ {label_id}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œì™¸ í›„ usable ìƒ˜í”Œ ìˆ˜: 838\n"
     ]
    }
   ],
   "source": [
    "# ë¼ë²¨ 3 (\"ê°í•˜\"), ë¼ë²¨ 6 (\"ì·¨í•˜\") ì œê±°\n",
    "filtered_data_list = [x for x in data_list if x[\"label\"] not in [3, 6]]\n",
    "print(f\"ì œì™¸ í›„ usable ìƒ˜í”Œ ìˆ˜: {len(filtered_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 586, Val: 168, Test: 84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test 10%\n",
    "train_val, test = train_test_split(\n",
    "    filtered_data_list,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=[x[\"label\"] for x in filtered_data_list]\n",
    ")\n",
    "\n",
    "# train:val = 7:2 â†’ val ë¹„ìœ¨ì€ 2/9\n",
    "train, val = train_test_split(\n",
    "    train_val,\n",
    "    test_size=2/9,\n",
    "    random_state=42,\n",
    "    stratify=[x[\"label\"] for x in train_val]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Train ë¶„í¬:\n",
      "ì¸ìš© (0): 93ê°œ\n",
      "ì¼ë¶€ì¸ìš© (1): 116ê°œ\n",
      "ê¸°ê° (2): 106ê°œ\n",
      "íŒŒê¸°í™˜ì†¡ (4): 146ê°œ\n",
      "ì›ì‹¬ìœ ì§€ (5): 43ê°œ\n",
      "íŒŒê¸°í™˜ì†¡, ì›ì‹¬ìœ ì§€ (7): 82ê°œ\n",
      "\n",
      "ğŸ“Š Val ë¶„í¬:\n",
      "ì¸ìš© (0): 27ê°œ\n",
      "ì¼ë¶€ì¸ìš© (1): 33ê°œ\n",
      "ê¸°ê° (2): 30ê°œ\n",
      "íŒŒê¸°í™˜ì†¡ (4): 42ê°œ\n",
      "ì›ì‹¬ìœ ì§€ (5): 13ê°œ\n",
      "íŒŒê¸°í™˜ì†¡, ì›ì‹¬ìœ ì§€ (7): 23ê°œ\n",
      "\n",
      "ğŸ“Š Test ë¶„í¬:\n",
      "ì¸ìš© (0): 13ê°œ\n",
      "ì¼ë¶€ì¸ìš© (1): 17ê°œ\n",
      "ê¸°ê° (2): 15ê°œ\n",
      "íŒŒê¸°í™˜ì†¡ (4): 21ê°œ\n",
      "ì›ì‹¬ìœ ì§€ (5): 6ê°œ\n",
      "íŒŒê¸°í™˜ì†¡, ì›ì‹¬ìœ ì§€ (7): 12ê°œ\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_label_distribution(data, label_map):\n",
    "    id_to_label = {v: k for k, v in label_map.items()}\n",
    "    counts = Counter([x[\"label\"] for x in data])\n",
    "    for label_id, count in sorted(counts.items()):\n",
    "        label_name = id_to_label.get(label_id, \"Unknown\")\n",
    "        print(f\"{label_name} ({label_id}): {count}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ“Š Train ë¶„í¬:\")\n",
    "print_label_distribution(train, label_map)\n",
    "print(\"\\nğŸ“Š Val ë¶„í¬:\")\n",
    "print_label_distribution(val, label_map)\n",
    "print(\"\\nğŸ“Š Test ë¶„í¬:\")\n",
    "print_label_distribution(test, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## [7] ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, \"train_data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"val_data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
